{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['Apple', 'and', 'orange', 'are', 'fruits', 'Apple', 'and', 'Google', 'are', 'IT', 'companies'], ['Hyderabad', 'and', 'Secunderabad', 'are', 'twin', 'cities', 'Vijayawada', 'and', 'Bejawada', 'are', 'names', 'for', 'same', 'city'], ['India', 'has', '29', 'states', 'Telangana', 'is', 'one', 'of', 'them']]\n"
     ]
    }
   ],
   "source": [
    "# create corpus - list of documents\n",
    "doc1 = \"Apple and orange are fruits. Apple and Google are IT companies.\"\n",
    "doc2 = \"Hyderabad and Secunderabad are twin cities. Vijayawada and Bejawada are names for same city\"\n",
    "doc3 = \"India has 29 states. Telangana is one of them\"\n",
    "# tokenize and exclude punctuation marks\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "token1 = tokenizer.tokenize(doc1)            # removes punctuation marks\n",
    "print(type(token1))\n",
    "token2 = tokenizer.tokenize(doc2)\n",
    "token3 = tokenizer.tokenize(doc3)\n",
    "sentence = list()\n",
    "sentence.append(token1)\n",
    "sentence.append(token2)\n",
    "sentence.append(token3)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    }
   ],
   "source": [
    "# create word2vec \n",
    "model = Word2Vec(sentence, sg = 0, min_count=1,size=5,window=5,workers=4,\n",
    "                 seed =10, alpha = 0.01, min_alpha=0.001,\n",
    "                 compute_loss=True, iter=50)         # size represents number of dimensions#\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1328.726806640625"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishnu Murthy Chakka\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04576896, -0.0460228 ,  0.08032094, -0.06570196, -0.0931375 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print word vector for any word\n",
    "model['Apple']\n",
    "#model.wv._getitem_('Apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishnu Murthy Chakka\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.02377045,  0.09000102,  0.03034329, -0.08768976,  0.00173387],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishnu Murthy Chakka\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.09084674, -0.09710447,  0.05933991, -0.07595898,  0.05437203],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['Google']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.55234265"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two words\n",
    "model.wv.similarity('Hyderabad','Secunderabad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35862145"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('India','Telangana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.098391965"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('Apple','orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19958498"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('Apple','Google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36182353"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('Apple','fruits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.26950532"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('Apple','IT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.085544378"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('Vijayawada','Bejawada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India', 0.7632635831832886)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['Apple', 'fruits'], negative=['orange'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'are', 'Apple', 'orange', 'fruits', 'Google', 'IT', 'companies', 'Hyderabad', 'Secunderabad', 'twin', 'cities', 'Vijayawada', 'Bejawada', 'names', 'for', 'same', 'city', 'India', 'has', '29', 'states', 'Telangana', 'is', 'one', 'of', 'them']\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# print all words\n",
    "print(model.wv.index2word)\n",
    "print(len(model.wv.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02612994,  0.00679522,  0.03088141, -0.04218544,  0.01178725,\n",
       "        -0.02711239,  0.00707143, -0.01302773, -0.02990203, -0.00189833],\n",
       "       [ 0.02507149, -0.04845297, -0.04416666,  0.01537336, -0.02588061,\n",
       "         0.03807232,  0.01327009, -0.04607012,  0.01074946, -0.03296657]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print vectors for all words\n",
    "model.wv.vectors[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip word and corresponding vector\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "#print(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35, 170)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build word2vec in two phases - first build vocabulary and train it\n",
    "model3 = Word2Vec(min_count=1,size=10)\n",
    "model3.build_vocab(sentence)\n",
    "print(model3.epochs)\n",
    "print(len(sentence))\n",
    "model3.train(sentence, total_words=len(sentence), epochs=model3.epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00998848 -0.00347426  0.01405674 -0.04747547 -0.03307188 -0.00033746\n",
      "  -0.0415465  -0.02504256 -0.01051377 -0.02789557]]\n"
     ]
    }
   ],
   "source": [
    "# print vectors for all words\n",
    "print(model3.wv.vectors[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Vishnu Murthy Chakka\\Knowledge Base\\Data Science\\Manuals\\Natural Language Processing\\glove\n"
     ]
    }
   ],
   "source": [
    "# read pre-trained vectors from glove\n",
    "import os\n",
    "os.chdir('C:/Vishnu Murthy Chakka/Knowledge Base/Data Science/Manuals/Natural Language Processing/glove')\n",
    "print(os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "#    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "#           for line in lines}\n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    w2v = {line.split()[0].decode(\"utf-8\"): np.array(line.split()[1:]).astype(float)\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.61038  -0.20757  -0.71951   0.89304   0.32482   0.76564   0.1814\n",
      " -0.33086   0.79173  -0.31664   0.011143  0.45412   1.5992    0.013494\n",
      " -0.093646  0.19245   0.251     1.1277   -1.0897   -0.42909  -1.1327\n",
      " -0.90465   0.5617   -0.058464  1.0007   -0.39017  -0.41665   0.73721\n",
      " -0.53824  -0.95993   0.67929  -0.59053   0.13408   0.54273  -0.36615\n",
      "  0.014978 -0.2496   -0.81088   0.078905 -0.97552  -0.66394  -0.18508\n",
      " -0.87174   0.30782   1.2839   -0.14884   0.62178  -1.509     0.14582\n",
      " -0.31682 ]\n",
      "[ 0.16142  -1.0424   -0.46239   0.77606   0.26464   0.31932  -0.76033\n",
      " -0.52584   0.38692   0.019854  0.13353   0.51689   1.1251    0.10543\n",
      "  0.055542 -0.18489   0.16058   0.30495  -1.6176   -0.88914  -2.0381\n",
      " -0.65167   1.6266   -0.50523   0.72486   0.061601  0.024211 -0.12515\n",
      " -0.26353  -1.3289    0.94717   0.71643  -0.24545  -0.13676   0.33316\n",
      "  0.15309  -0.40259  -0.22679   0.28291  -0.69532  -1.3783   -0.61586\n",
      " -0.70103   1.3187    1.043     0.77337   0.49553  -0.91989  -0.28027\n",
      " -0.45603 ]\n"
     ]
    }
   ],
   "source": [
    "print(w2v['frog'])\n",
    "print(w2v['frogs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Vishnu Murthy Chakka\\Knowledge Base\\Data Science\\Manuals\\Natural Language Processing\\google\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained vectors from google - 300 dimensions i.e. a word is represented by array of 300 numbers\n",
    "import os\n",
    "os.chdir('C:/Vishnu Murthy Chakka/Knowledge Base/Data Science/Manuals/Natural Language Processing/google')\n",
    "print(os.getcwd())\n",
    "from gensim.models import KeyedVectors \n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01,\n",
       "       -8.44726562e-02,  5.73730469e-02,  5.85937500e-02, -8.25195312e-02,\n",
       "       -1.53808594e-02, -6.34765625e-02,  1.79687500e-01, -4.23828125e-01,\n",
       "       -2.25830078e-02, -1.66015625e-01, -2.51464844e-02,  1.07421875e-01,\n",
       "       -1.99218750e-01,  1.59179688e-01, -1.87500000e-01, -1.20117188e-01,\n",
       "        1.55273438e-01, -9.91210938e-02,  1.42578125e-01, -1.64062500e-01,\n",
       "       -8.93554688e-02,  2.00195312e-01, -1.49414062e-01,  3.20312500e-01,\n",
       "        3.28125000e-01,  2.44140625e-02, -9.71679688e-02, -8.20312500e-02,\n",
       "       -3.63769531e-02, -8.59375000e-02, -9.86328125e-02,  7.78198242e-03,\n",
       "       -1.34277344e-02,  5.27343750e-02,  1.48437500e-01,  3.33984375e-01,\n",
       "        1.66015625e-02, -2.12890625e-01, -1.50756836e-02,  5.24902344e-02,\n",
       "       -1.07421875e-01, -8.88671875e-02,  2.49023438e-01, -7.03125000e-02,\n",
       "       -1.59912109e-02,  7.56835938e-02, -7.03125000e-02,  1.19140625e-01,\n",
       "        2.29492188e-01,  1.41601562e-02,  1.15234375e-01,  7.50732422e-03,\n",
       "        2.75390625e-01, -2.44140625e-01,  2.96875000e-01,  3.49121094e-02,\n",
       "        2.42187500e-01,  1.35742188e-01,  1.42578125e-01,  1.75781250e-02,\n",
       "        2.92968750e-02, -1.21582031e-01,  2.28271484e-02, -4.76074219e-02,\n",
       "       -1.55273438e-01,  3.14331055e-03,  3.45703125e-01,  1.22558594e-01,\n",
       "       -1.95312500e-01,  8.10546875e-02, -6.83593750e-02, -1.47094727e-02,\n",
       "        2.14843750e-01, -1.21093750e-01,  1.57226562e-01, -2.07031250e-01,\n",
       "        1.36718750e-01, -1.29882812e-01,  5.29785156e-02, -2.71484375e-01,\n",
       "       -2.98828125e-01, -1.84570312e-01, -2.29492188e-01,  1.19140625e-01,\n",
       "        1.53198242e-02, -2.61718750e-01, -1.23046875e-01, -1.86767578e-02,\n",
       "       -6.49414062e-02, -8.15429688e-02,  7.86132812e-02, -3.53515625e-01,\n",
       "        5.24902344e-02, -2.45361328e-02, -5.43212891e-03, -2.08984375e-01,\n",
       "       -2.10937500e-01, -1.79687500e-01,  2.42187500e-01,  2.57812500e-01,\n",
       "        1.37695312e-01, -2.10937500e-01, -2.17285156e-02, -1.38671875e-01,\n",
       "        1.84326172e-02, -1.23901367e-02, -1.59179688e-01,  1.61132812e-01,\n",
       "        2.08007812e-01,  1.03027344e-01,  9.81445312e-02, -6.83593750e-02,\n",
       "       -8.72802734e-03, -2.89062500e-01, -2.14843750e-01, -1.14257812e-01,\n",
       "       -2.21679688e-01,  4.12597656e-02, -3.12500000e-01, -5.59082031e-02,\n",
       "       -9.76562500e-02,  5.81054688e-02, -4.05273438e-02, -1.73828125e-01,\n",
       "        1.64062500e-01, -2.53906250e-01, -1.54296875e-01, -2.31933594e-02,\n",
       "       -2.38281250e-01,  2.07519531e-02, -2.73437500e-01,  3.90625000e-03,\n",
       "        1.13769531e-01, -1.73828125e-01,  2.57812500e-01,  2.35351562e-01,\n",
       "        5.22460938e-02,  6.83593750e-02, -1.75781250e-01,  1.60156250e-01,\n",
       "       -5.98907471e-04,  5.98144531e-02, -2.11914062e-01, -5.54199219e-02,\n",
       "       -7.51953125e-02, -3.06640625e-01,  4.27734375e-01,  5.32226562e-02,\n",
       "       -2.08984375e-01, -5.71289062e-02, -2.09960938e-01,  3.29589844e-02,\n",
       "        1.05468750e-01, -1.50390625e-01, -9.37500000e-02,  1.16699219e-01,\n",
       "        6.44531250e-02,  2.80761719e-02,  2.41210938e-01, -1.25976562e-01,\n",
       "       -1.00585938e-01, -1.22680664e-02, -3.26156616e-04,  1.58691406e-02,\n",
       "        1.27929688e-01, -3.32031250e-02,  4.07714844e-02, -1.31835938e-01,\n",
       "        9.81445312e-02,  1.74804688e-01, -2.36328125e-01,  5.17578125e-02,\n",
       "        1.83593750e-01,  2.42919922e-02, -4.31640625e-01,  2.46093750e-01,\n",
       "       -3.03955078e-02, -2.47802734e-02, -1.17187500e-01,  1.61132812e-01,\n",
       "       -5.71289062e-02,  1.16577148e-02,  2.81250000e-01,  4.27734375e-01,\n",
       "        4.56542969e-02,  1.01074219e-01, -3.95507812e-02,  1.77001953e-02,\n",
       "       -8.98437500e-02,  1.35742188e-01,  2.08007812e-01,  1.88476562e-01,\n",
       "       -1.52343750e-01, -2.37304688e-01, -1.90429688e-01,  7.12890625e-02,\n",
       "       -2.46093750e-01, -2.61718750e-01, -2.34375000e-01, -1.45507812e-01,\n",
       "       -1.17187500e-02, -1.50390625e-01, -1.13281250e-01,  1.82617188e-01,\n",
       "        2.63671875e-01, -1.37695312e-01, -4.58984375e-01, -4.68750000e-02,\n",
       "       -1.26953125e-01, -4.22363281e-02, -1.66992188e-01,  1.26953125e-01,\n",
       "        2.59765625e-01, -2.44140625e-01, -2.19726562e-01, -8.69140625e-02,\n",
       "        1.59179688e-01, -3.78417969e-02,  8.97216797e-03, -2.77343750e-01,\n",
       "       -1.04980469e-01, -1.75781250e-01,  2.28515625e-01, -2.70996094e-02,\n",
       "        2.85156250e-01, -2.73437500e-01,  1.61132812e-02,  5.90820312e-02,\n",
       "       -2.39257812e-01,  1.77734375e-01, -1.34765625e-01,  1.38671875e-01,\n",
       "        3.53515625e-01,  1.22070312e-01,  1.43554688e-01,  9.22851562e-02,\n",
       "        2.29492188e-01, -3.00781250e-01, -4.88281250e-02, -1.79687500e-01,\n",
       "        2.96875000e-01,  1.75781250e-01,  4.80957031e-02, -3.38745117e-03,\n",
       "        7.91015625e-02, -2.38281250e-01, -2.31445312e-01,  1.66015625e-01,\n",
       "       -2.13867188e-01, -7.03125000e-02, -7.56835938e-02,  1.96289062e-01,\n",
       "       -1.29882812e-01, -1.05957031e-01, -3.53515625e-01, -1.16699219e-01,\n",
       "       -5.10253906e-02,  3.39355469e-02, -1.43554688e-01, -3.90625000e-03,\n",
       "        1.73828125e-01, -9.96093750e-02, -1.66015625e-01, -8.54492188e-02,\n",
       "       -3.82812500e-01,  5.90820312e-02, -6.22558594e-02,  8.83789062e-02,\n",
       "       -8.88671875e-02,  3.28125000e-01,  6.83593750e-02, -1.91406250e-01,\n",
       "       -8.35418701e-04,  1.04003906e-01,  1.52343750e-01, -1.53350830e-03,\n",
       "        4.16015625e-01, -3.32031250e-02,  1.49414062e-01,  2.42187500e-01,\n",
       "       -1.76757812e-01, -4.93164062e-02, -1.24511719e-01,  1.25976562e-01,\n",
       "        1.74804688e-01,  2.81250000e-01, -1.80664062e-01,  1.03027344e-01,\n",
       "       -2.75390625e-01,  2.61718750e-01,  2.46093750e-01, -4.71191406e-02,\n",
       "        6.25000000e-02,  4.16015625e-01, -3.55468750e-01,  2.22656250e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0088713523"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(model['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766401\n"
     ]
    }
   ],
   "source": [
    "#printing similarity index\n",
    "print(model.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.663204\n"
     ]
    }
   ],
   "source": [
    "#printing similarity index\n",
    "print(model.similarity('Delhi', 'India'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.643999\n"
     ]
    }
   ],
   "source": [
    "#printing similarity index\n",
    "print(model.similarity('Delhi', 'Islamabad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.447594\n"
     ]
    }
   ],
   "source": [
    "#printing similarity index\n",
    "print(model.similarity('Delhi', 'London'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained vectors from google\n",
    "from gensim.test.utils import datapath\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)  # C text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wv_from_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.56110603, -1.97569799,  1.66395497, -1.23224604,  0.75475103,\n",
       "        0.98576403,  2.26144099, -0.59829003, -0.47433099, -1.41610503], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_text['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.478865  , -2.14878297,  1.82030797, -1.33254898,  0.86115199,\n",
       "        1.06142497,  2.51318002, -0.62415099, -0.508928  , -1.59284794], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_text['for']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'dog' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0e3607c11264>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwv_from_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dog'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'dog' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "wv_from_text['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "#        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# use keras for tokenization\n",
    "from keras.preprocessing import text, sequence\n",
    "doc1 = \"Apple and orange are fruits. Apple and Google are IT companies.\"\n",
    "doc2 = \"Hyderabad and Secunderabad are twin cities. Vijayawada and Bejawada are names for same city\"\n",
    "doc3 = \"India has 29 states. Telangana is one of them\"\n",
    "corpus = list()\n",
    "corpus.append(doc1)\n",
    "corpus.append(doc2)\n",
    "corpus.append(doc3)\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(corpus)\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('and', 1), ('are', 2), ('apple', 3), ('orange', 4), ('fruits', 5), ('google', 6), ('it', 7), ('companies', 8), ('hyderabad', 9), ('secunderabad', 10), ('twin', 11), ('cities', 12), ('vijayawada', 13), ('bejawada', 14), ('names', 15), ('for', 16), ('same', 17), ('city', 18), ('india', 19), ('has', 20), ('29', 21), ('states', 22), ('telangana', 23), ('is', 24), ('one', 25), ('of', 26), ('them', 27)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'are': 2, 'apple': 3, 'orange': 4, 'fruits': 5, 'google': 6, 'it': 7, 'companies': 8, 'hyderabad': 9, 'secunderabad': 10, 'twin': 11, 'cities': 12, 'vijayawada': 13, 'bejawada': 14, 'names': 15, 'for': 16, 'same': 17, 'city': 18, 'india': 19, 'has': 20, '29': 21, 'states': 22, 'telangana': 23, 'is': 24, 'one': 25, 'of': 26, 'them': 27}\n"
     ]
    }
   ],
   "source": [
    "# get index of tokens created\n",
    "print(word_index)          # this is dictionary/vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 4, 2, 5, 3, 1, 6, 2, 7, 8], [9, 1, 10, 2, 11, 12, 13, 1, 14, 2, 15, 16, 17, 18], [19, 20, 21, 22, 23, 24, 25, 26, 27]]\n"
     ]
    }
   ],
   "source": [
    "# convert words/tokens to index\n",
    "print(token.texts_to_sequences(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  3  1  4  2  5  3  1  6  2  7  8]\n",
      " [ 0  0  0  0  0  0  9  1 10  2 11 12 13  1 14  2 15 16 17 18]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 19 20 21 22 23 24 25 26 27]]\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "input = sequence.pad_sequences(token.texts_to_sequences(corpus), maxlen=20)\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        0.        0.       ...,  0.        0.        0.      ]\n",
      " [ 0.26818   0.14346  -0.27877  ..., -0.6321   -0.25028  -0.38097 ]\n",
      " [ 0.96193   0.012516  0.21733  ...,  0.14032  -0.38468  -0.38712 ]\n",
      " ..., \n",
      " [ 0.31474   0.41662   0.1348   ..., -0.20526   0.07009  -0.11568 ]\n",
      " [ 0.70853   0.57088  -0.4716   ..., -0.22562  -0.093918 -0.80375 ]\n",
      " [ 0.64642  -0.556     0.47038  ..., -0.35831  -0.10995  -0.447   ]]\n"
     ]
    }
   ],
   "source": [
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v.keys():\n",
    "        embedding_matrix[i] = w2v[word]\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
